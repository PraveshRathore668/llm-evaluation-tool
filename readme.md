# LLM Multi-Round Evaluation Tool

A Streamlit application for evaluating multiple Large Language Models across multiple rounds of testing.

## Features
- Multi-round evaluation (default 5 rounds)
- Peer evaluation between LLMs
- Statistical analysis and ranking
- Support for English and Hindi

## Setup
1. Clone this repository
2. Install requirements: `pip install -r requirements.txt`
3. Run the app: `streamlit run app.py`

## Usage
1. Configure LLMs and test cases in the sidebar
2. Run Phase 1 to get LLM responses
3. Run Phase 2 for peer evaluations
4. View results in Phase 3